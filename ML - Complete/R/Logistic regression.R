# what difference did u notice in python and R.
setwd("F:/Machine-Learning/ML - Complete/2 - Logistic Regression")

data <- read.csv("Affairs.csv")
data$affairs <- NULL
set.seed(22)
# To generate integers WITHOUT replacement:
trainIndex<-sample(1:6366,5000,replace = F)
train <- data[trainIndex,]
test <- data[-trainIndex,]
frmla <- "affair ~ rate_marriage + age + yrs_married + children + religious + educ + occupation + occupation_husb"
model1 <- glm(formula = frmla,data = train,family = "binomial") # Gauusian
#model1 <- glm(formula = frmla,data = train,family = binomial(link='logit'))
summary(model1)
# Residual deviance is the measure of lack of it
# Null deviance: some times model ignores some observation...so the measure of deviance on that reduced set.
frmla <- "affair ~ rate_marriage + age + yrs_married + religious + educ + occupation + occupation_husb"
model2 <- glm(formula = frmla,data = train,family = "binomial") # Gauusian
summary(model2)
frmla <- "affair ~ rate_marriage + age + yrs_married + religious + educ + occupation "
model3 <- glm(formula = frmla,data = train,family = "binomial") # Gauusian
summary(model3)
# anova(model1, test="Chisq")
test$pred <- predict(model1,newdata = test,type = 'response')
test$predicted <- ifelse(test$pred >= 0.5,1,0)
ConfMatrix <- as.data.frame(table(test$affair,test$predicted))
misClasificError <- mean(test$affair != test$predicted)

#true positives (TP): These are cases in which we predicted yes (they have the disease), and they do have the disease.
#true negatives (TN): We predicted no, and they don't have the disease.
#false positives (FP): We predicted yes, but they don't actually have the disease. (Also known as a "Type I error.")
#false negatives (FN): He does have a disease, model predicted No (Also known as a "Type II error.")
ConfMatrix$names <- c("TN","FN","FP","TP")
# accuracy = (TN+TP)/Total
Accuracy <- (ConfMatrix$Freq[1] + ConfMatrix$Freq[4])/1366 
# "Sensitivity" or "Recall" = (TP/FN+TP)
#When it's actually yes, how often does it predict yes?
recall <- ConfMatrix$Freq[4]/(ConfMatrix$Freq[2]+ConfMatrix$Freq[4])
#Specificity: TN/(TN+FP)
#When it's actually no, how often does it predict no?
specifi <-  ConfMatrix$Freq[1]/(ConfMatrix$Freq[1] + ConfMatrix$Freq[3])


# Precision TP/(TP+FP))
#When it predicts yes, how often is it correct? , they talk same
precisi <- ConfMatrix$Freq[4]/(ConfMatrix$Freq[4] + ConfMatrix$Freq[3])
  

library(ROCR)
pr <- prediction(test$predicted, test$affair)
prf <- performance(pr, measure = "tpr", x.measure = "fpr") 
# ROC curve is generated by ploting true positive rates and false positive rates
plot(prf)
# to measure the area under the curve  
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
