{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Sample.csv\")\n",
    "data.columns\n",
    "for i in data.columns:\n",
    "    if data[i].dtype == 'object':\n",
    "        le.fit(data[i])\n",
    "        data[i] = le.transform(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = data[[\"current\",\"powerfactor\",\"activepower\",\"reactivepower\",\"apparentpower\",\"activefundamentalpower\",\"activeharmonicpower\",\"meanphaseangle\"]]\n",
    "y = data.combinations\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=10,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=99, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# max_depth can be controlled for pruning\n",
    "model = DecisionTreeClassifier(criterion = 'entropy',min_samples_split=20, random_state=99, max_depth=10)\n",
    "model = model.fit(X_train,y_train)\n",
    "print(model)\n",
    "Y_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('DecisionTrees.pickle','wb') as f:\n",
    "    pickle.dump(model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickle_in = open('DecisionTrees.pickle','rb')\n",
    "model = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Importance\n",
      "current                   0.184855\n",
      "powerfactor               0.016205\n",
      "activepower               0.114206\n",
      "reactivepower             0.215876\n",
      "apparentpower             0.058358\n",
      "activefundamentalpower    0.169705\n",
      "activeharmonicpower       0.008748\n",
      "meanphaseangle            0.232048\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(model.feature_importances_,columns = ['Importance'],index = X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "----------------------------------------------\n",
      "                        Importance\n",
      "current                   0.155075\n",
      "powerfactor               0.106332\n",
      "activepower               0.140573\n",
      "reactivepower             0.137823\n",
      "apparentpower             0.134682\n",
      "activefundamentalpower    0.137424\n",
      "activeharmonicpower       0.066822\n",
      "meanphaseangle            0.121270\n"
     ]
    }
   ],
   "source": [
    "# max_features: max num of features Random Forest is allowed to try in individual tree\n",
    "# n_estimators: Max num of trees to build, Higher num of trees give you better performance but makes slower\n",
    "# n_jobs: parameter tells the engine how many processors is it allowed to use, -1 no restriction\n",
    "model= RandomForestClassifier(n_estimators=1000)\n",
    "model = model.fit(X_train,y_train)\n",
    "print(model)\n",
    "print(\"----------------------------------------------\")\n",
    "print(pd.DataFrame(model.feature_importances_,columns = ['Importance'],index = X_train.columns))\n",
    "Y_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=100, random_state=None)\n",
      "----------------------------------------------\n",
      "                        Importance\n",
      "current                       0.06\n",
      "powerfactor                   0.09\n",
      "activepower                   0.04\n",
      "reactivepower                 0.09\n",
      "apparentpower                 0.08\n",
      "activefundamentalpower        0.12\n",
      "activeharmonicpower           0.00\n",
      "meanphaseangle                0.52\n"
     ]
    }
   ],
   "source": [
    "model3 = AdaBoostClassifier(n_estimators=100)\n",
    "model3 = model3.fit(X_train,y_train)\n",
    "print(model3)\n",
    "print(\"----------------------------------------------\")\n",
    "print(pd.DataFrame(model3.feature_importances_,columns = ['Importance'],index = X_train.columns))\n",
    "Y_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(init=None, learning_rate=1.0, loss='deviance',\n",
      "              max_depth=1, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=0, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "----------------------------------------------\n",
      "                        Importance\n",
      "current                   0.119286\n",
      "powerfactor               0.032857\n",
      "activepower               0.044762\n",
      "reactivepower             0.309048\n",
      "apparentpower             0.064048\n",
      "activefundamentalpower    0.049048\n",
      "activeharmonicpower       0.119286\n",
      "meanphaseangle            0.238095\n"
     ]
    }
   ],
   "source": [
    "model4= GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "model4 = model4.fit(X_train,y_train)\n",
    "print(model4)\n",
    "print(\"----------------------------------------------\")\n",
    "print(pd.DataFrame(model4.feature_importances_,columns = ['Importance'],index = X_train.columns))\n",
    "Y_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[73  0  0 ...,  0  0  0]\n",
      " [ 0 37  0 ...,  0  0  0]\n",
      " [ 0  0 39 ...,  0  0  0]\n",
      " ..., \n",
      " [ 0  0  0 ..., 27  0  0]\n",
      " [ 0  0  0 ...,  0 36  0]\n",
      " [ 0  0  0 ...,  0  0 29]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        73\n",
      "          1       1.00      1.00      1.00        37\n",
      "          2       1.00      1.00      1.00        39\n",
      "          3       0.98      1.00      0.99        44\n",
      "          4       1.00      1.00      1.00        35\n",
      "          5       1.00      1.00      1.00        62\n",
      "          6       1.00      1.00      1.00        32\n",
      "          7       1.00      1.00      1.00        37\n",
      "          8       1.00      1.00      1.00        27\n",
      "          9       1.00      1.00      1.00        37\n",
      "         10       1.00      1.00      1.00        36\n",
      "         11       1.00      1.00      1.00        35\n",
      "         12       1.00      1.00      1.00        44\n",
      "         13       1.00      1.00      1.00        33\n",
      "         14       1.00      1.00      1.00        40\n",
      "         15       1.00      1.00      1.00        31\n",
      "         16       1.00      1.00      1.00        34\n",
      "         17       1.00      0.97      0.98        30\n",
      "         18       1.00      1.00      1.00        36\n",
      "         19       1.00      1.00      1.00        39\n",
      "         20       1.00      1.00      1.00        37\n",
      "         21       1.00      1.00      1.00        35\n",
      "         22       1.00      1.00      1.00        35\n",
      "         23       1.00      1.00      1.00        32\n",
      "         24       1.00      1.00      1.00        36\n",
      "         25       1.00      1.00      1.00        38\n",
      "         26       1.00      1.00      1.00        33\n",
      "         27       1.00      1.00      1.00        39\n",
      "         28       1.00      1.00      1.00        30\n",
      "         29       1.00      1.00      1.00        28\n",
      "         30       1.00      1.00      1.00        39\n",
      "         31       1.00      1.00      1.00        31\n",
      "         32       1.00      1.00      1.00        39\n",
      "         33       1.00      1.00      1.00        41\n",
      "         34       1.00      1.00      1.00        33\n",
      "         35       1.00      1.00      1.00        30\n",
      "         36       1.00      1.00      1.00        33\n",
      "         37       1.00      1.00      1.00        32\n",
      "         38       1.00      1.00      1.00        33\n",
      "         39       1.00      1.00      1.00        27\n",
      "         40       1.00      1.00      1.00        36\n",
      "         41       1.00      1.00      1.00        29\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1527\n",
      "\n",
      "0.999345121153\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# Confusion Matrix\n",
    "print(metrics.confusion_matrix(y_test, Y_test))\n",
    "print(metrics.classification_report(y_test, Y_test))\n",
    "print(metrics.accuracy_score(y_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.0091683038637852\n",
      "Mean Squared Error: 0.12835625409299278\n",
      "R - Squared value: 0.9991588209138685\n",
      "What percent of predictions are same: 0.9993451211525868\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, jaccard_similarity_score, roc_auc_score\n",
    "\n",
    "#ev = explained_variance_score(y_test, Y_test, multioutput='uniform_average')\n",
    "# Best possible score is 1.0, lower values are worse.\n",
    "#print(\"Explained Variance Score: {}\". format(ev))\n",
    "\n",
    "mae = mean_absolute_error(y_test, Y_test, multioutput='uniform_average')\n",
    "# MAE output is non-negative floating point. The best value is 0.0.\n",
    "print(\"Mean Absolute Error: {}\".format(mae))\n",
    "\n",
    "mse = mean_squared_error(y_test, Y_test, multioutput='uniform_average')\n",
    "# MAE output is non-negative floating point. The best value is 0.0.\n",
    "print(\"Mean Squared Error: {}\".format(mse))\n",
    "\n",
    "r2 = r2_score(y_test, Y_test)\n",
    "# R^2 (coefficient of determination) regression score function.\n",
    "# Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always \n",
    "# predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.\n",
    "print(\"R - Squared value: {}\".format(r2))\n",
    "\n",
    "print('What percent of predictions are same: {}'.format(jaccard_similarity_score(y_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
